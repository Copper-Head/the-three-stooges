{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation test of LK-C-Model\n",
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.model import Model\n",
    "from conllutil import CoNLLData\n",
    "from itertools import chain\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from network import *\n",
    "from numpy import array, hstack, load, save, vstack, zeros\n",
    "from os import path\n",
    "from pandas import factorize\n",
    "from random import randint\n",
    "from scipy.stats import pearsonr\n",
    "from stoogeplot import hinton_diagram\n",
    "from theano import function\n",
    "from theano.tensor.sharedvar import SharedVariable\n",
    "from theano.tensor import matrix, TensorType\n",
    "from util import StateComputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA = .05\n",
    "MODEL_FILE = './models/hdt/hdt-ncs-eos-np-35-7-1.pkl'\n",
    "IX_2_TOK_FILE = './data/hdt-ncs-eos-np-35-7-1_ix2tok.npy'\n",
    "HDT_DIR = '../datasets/hdt/hamburg-dependency-treebank-conll/'\n",
    "NP_FOLDER = './data/np'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ix2tok = load(IX_2_TOK_FILE).item()\n",
    "nt = Network(NetworkType.LSTM, input_dim=len(ix2tok))\n",
    "nt.set_parameters(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(nt.generator.generate(n_steps=nt.x.shape[0], batch_size=nt.x.shape[1]))\n",
    "param_dict = model.get_parameter_dict()\n",
    "init_state_0 = param_dict['/sequencegenerator/with_fake_attention/transition/layer#0.initial_state#0']\n",
    "init_state_1 = param_dict['/sequencegenerator/with_fake_attention/transition/layer#1.initial_state#1']\n",
    "init_state_2 = param_dict['/sequencegenerator/with_fake_attention/transition/layer#2.initial_state#2']\n",
    "init_cells_0 = param_dict['/sequencegenerator/with_fake_attention/transition/layer#0.initial_cells']\n",
    "init_cells_1 = param_dict['/sequencegenerator/with_fake_attention/transition/layer#1.initial_cells']\n",
    "init_cells_2 = param_dict['/sequencegenerator/with_fake_attention/transition/layer#2.initial_cells']\n",
    "reset_values = {\n",
    "    0: (init_state_0.get_value(), init_cells_0.get_value()),\n",
    "    1: (init_state_1.get_value(), init_cells_1.get_value()),\n",
    "    2: (init_state_2.get_value(), init_cells_2.get_value())\n",
    "}\n",
    "gen_func = model.get_theano_function(allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tok2ix = {v: k for k, v in ix2tok.items()}\n",
    "sc = StateComputer(nt.cost_model, tok2ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we continue ... save the model in a lightweight format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save(path.join(NP_FOLDER, 'param_dict.npy'), array(param_dict))\n",
    "#for k in param_dict:\n",
    "#    save(path.join(NP_FOLDER, k.replace('/', '-')), param_dict[k].get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reset_generator():\n",
    "    init_state_0 = reset_values[0][0]\n",
    "    init_cells_0 = reset_values[0][1]\n",
    "    init_state_1 = reset_values[1][0]\n",
    "    init_cells_1 = reset_values[1][1]\n",
    "    init_state_2 = reset_values[2][0]\n",
    "    init_cells_2 = reset_values[2][1]\n",
    "    \n",
    "def init_zero():\n",
    "    # note sure this is always a good idea\n",
    "    d = init_state_0.get_value().shape[0]\n",
    "    dt = 'float32'\n",
    "    init_state_0.set_value(zeros(d, dtype=dt))\n",
    "    init_cells_0.set_value(zeros(d, dtype=dt))\n",
    "    init_state_1.set_value(zeros(d, dtype=dt))\n",
    "    init_cells_1.set_value(zeros(d, dtype=dt))\n",
    "    init_state_2.set_value(zeros(d, dtype=dt))\n",
    "    init_cells_2.set_value(zeros(d, dtype=dt))\n",
    "    \n",
    "def generate_sequence(start, reset_func):\n",
    "    \n",
    "    seq = [start]\n",
    "    ix = array([[tok2ix[start]]])\n",
    "    while not seq[-1] == '<EOS>':\n",
    "        state_0, cells_0, state_1, cells_1, state_2, cells_2, ix, costs = gen_func(ix)\n",
    "        init_state_0.set_value(state_0[0][0])\n",
    "        init_cells_0.set_value(cells_0[0][0])\n",
    "        init_state_1.set_value(state_1[0][0])\n",
    "        init_cells_1.set_value(cells_1[0][0])\n",
    "        init_state_2.set_value(state_2[0][0])\n",
    "        init_cells_2.set_value(cells_2[0][0])\n",
    "        seq.append(ix2tok[ix[0][0]])\n",
    "    \n",
    "    reset_func()\n",
    "    \n",
    "    return ' '.join(seq[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(generate_sequence('ein', reset_generator))  # good results 500 - 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['begleitet',\n",
       "  'von',\n",
       "  'marktgerüchten',\n",
       "  'über',\n",
       "  'den',\n",
       "  'bevorstehenden',\n",
       "  'konkurs',\n",
       "  'von',\n",
       "  'amazon',\n",
       "  'setzt',\n",
       "  'die',\n",
       "  'aktie',\n",
       "  'des',\n",
       "  'online-händlers',\n",
       "  'ihre',\n",
       "  'talfahrt',\n",
       "  'fort'],\n",
       " ['an',\n",
       "  'der',\n",
       "  'nasdaq',\n",
       "  'rutschte',\n",
       "  'das',\n",
       "  'papier',\n",
       "  'am',\n",
       "  'gestrigen',\n",
       "  'mittwoch',\n",
       "  'kurz',\n",
       "  'sogar',\n",
       "  'unter',\n",
       "  'die',\n",
       "  'marke',\n",
       "  'von',\n",
       "  '10',\n",
       "  'us-dollar',\n",
       "  'in',\n",
       "  'frankfurt',\n",
       "  'pendelt',\n",
       "  'sie',\n",
       "  'am',\n",
       "  'heutigen',\n",
       "  'donnerstag',\n",
       "  'zwischen',\n",
       "  'und',\n",
       "  'euro']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.wordsequences()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check correlations with POS\n",
    "## Step 1: Read all sentences from PART_A and store activations\n",
    "\n",
    "__comment:__  Only part A is used, since this one's annotations are handmade and checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eos_ix = tok2ix['<EOS>']\n",
    "cd = CoNLLData(HDT_DIR, ['part_A.conll'], tok2ix, word_transform=str.lower, lazy_loading=True, min_len=7, max_len=35)\n",
    "sentences_ix = [[tok2ix[seq[i]] for i in range(len(seq))] + [eos_ix] for seq in cd.wordsequences()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell_name = sc.state_var_names[2]\n",
    "# for testing purposes only read two sequences, try again later with more\n",
    "activations = sc.read_single_sequence(sentences_ix[0])[cell_name][1:,:]\n",
    "activations = vstack((activations, sc.read_single_sequence(sentences_ix[1])[cell_name][1:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlate for each pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to keep computation low, we just do in for a small set, there is no time left to check it anyway\n",
    "# tagset = set(chain(*cd.possequences()))\n",
    "tagset = {'ART', 'ADJA', 'NN'}\n",
    "pos_corrs = {}\n",
    "for tag in tagset:\n",
    "    crlist = []\n",
    "    for activation in activations.transpose():\n",
    "        \"\"\"\n",
    "        NOTE: activation is the concatenated activation of a particular cell of all sentences\n",
    "        Otherwise there wouldn't be enough data points for sentences with 7 <= length <= 35\n",
    "        ---\n",
    "        This is not the cleanest imaginable approach, since the activations between sequences\n",
    "        are not related, but it still might provide some insights\n",
    "        \"\"\"\n",
    "        pc = pearsonr(activation, [1 if ptag == tag else 0 for ptag in chain(*cd.possequences()[0:2])])\n",
    "        crlist.append(pc[0] if pc[1] < ALPHA else 0.0)\n",
    "    pos_corrs[tag] = array(crlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract 10 highest correlations for POS-Tags ART, NN, ADJA, since they might be relevant for the DET relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_corr_ix = {}\n",
    "for tag in tagset:\n",
    "    top_corr_ix[tag] = abs(pos_corrs[tag]).argsort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot that for 50 tokens, 10 strongest cells only\n",
    "\n",
    "__NOTE:__ The plots are aligned this time, i. e. a displayed activation occured after reading the word given in the label below!\n",
    "\n",
    "#### ART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(30, 50))\n",
    "xlabels = [tpl[1]+' / '+tpl[4] for tpl in chain(*cd.sequences()[:2])]  # TODO remove index, just debugging!\n",
    "x = activations[:,top_corr_ix['ART'][-10:]]\n",
    "ax = plt.gca()\n",
    "h = hinton_diagram(x, ax=ax)\n",
    "ax.set_xticks(range(len(xlabels)))\n",
    "ax.set_xticklabels(xlabels, rotation=90, fontdict={'fontsize': 22})\n",
    "f.subplots_adjust(bottom=.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
